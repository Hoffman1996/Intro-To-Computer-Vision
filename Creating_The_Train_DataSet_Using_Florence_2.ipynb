{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "azca-Akf_oAv"
      },
      "outputs": [],
      "source": [
        "# # Install Florence-2 and dependencies\n",
        "!pip install openxlab timm opencv-python pillow open_clip_torch --quiet\n",
        "!pip install transformers==4.49.0 --quiet\n",
        "!pip install pydantic==2.10.6 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "import requests\n",
        "import copy\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import matplotlib.patches as patches\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "QTN1Ra2-n_sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kF4HoU_C_zDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FLICKR_DIR = \"/content/drive/MyDrive/Intro to computer vision/Final/flickr30k_images\"\n",
        "list_of_images_paths = [os.path.join(FLICKR_DIR, f) for f in os.listdir(FLICKR_DIR) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "print(f\"Found {len(list_of_images_paths)} images.\")"
      ],
      "metadata": {
        "id": "0Y5aQ_I7GQL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'microsoft/Florence-2-large'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().cuda()\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "o2lR4hshqG8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_example(image, task_prompt = \"<OD>\", text_input=None):\n",
        "    if text_input is None:\n",
        "        prompt = task_prompt\n",
        "    else:\n",
        "        prompt = task_prompt + text_input\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to('cuda', torch.float16)\n",
        "    generated_ids = model.generate(\n",
        "      input_ids=inputs[\"input_ids\"].cuda(),\n",
        "      pixel_values=inputs[\"pixel_values\"].cuda(),\n",
        "      max_new_tokens=1024,\n",
        "      early_stopping=False,\n",
        "      do_sample=False,\n",
        "      num_beams=3,\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "    parsed_answer = processor.post_process_generation(\n",
        "        generated_text,\n",
        "        task=task_prompt,\n",
        "        image_size=(image.width, image.height)\n",
        "    )\n",
        "\n",
        "    return parsed_answer"
      ],
      "metadata": {
        "id": "XtLZHqpgoBHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_labels = [\"person\", \"man\", \"woman\", \"boy\", \"girl\", \"dog\", \"cat\", \"horse\"]\n",
        "def plot_bbox(image, data):\n",
        "   # Create a figure and axes\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Plot each bounding box\n",
        "    for bbox, label in zip(data['bboxes'], data['labels']):\n",
        "      if label in target_labels:\n",
        "        if label in ['cat', 'dog', 'hosre']:\n",
        "          label = 'pet'\n",
        "        if label in ['man', 'woman', 'boy', 'girl']:\n",
        "          label = 'person'\n",
        "        # Unpack the bounding box coordinates\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        # Add the rectangle to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        # Annotate the label\n",
        "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "    # Remove the axis ticks and labels\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cjGIn3KqqfWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code for inference on a single image:**"
      ],
      "metadata": {
        "id": "ezcDCBukt31R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image_path = random.choice(list_of_images_paths)  # randomly pick image\n",
        "image = Image.open(sample_image_path).convert(\"RGB\")\n",
        "task_prompt = \"<OD>\"\n",
        "\n",
        "result = run_example(image, task_prompt)\n",
        "print(result)\n",
        "\n",
        "plot_bbox(image, result[\"<OD>\"])"
      ],
      "metadata": {
        "id": "2JV2MBouvaYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code for creating the train set:**"
      ],
      "metadata": {
        "id": "kVDj8tp5uHa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== CONFIGURATION ==========\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Intro to computer vision/Final/train_validation_images\"\n",
        "BATCH_SIZE = 50\n",
        "VALIDATION_SPLIT = 0.15\n",
        "DESIRED_LABELS = [\"person\", \"pet\"]\n",
        "MIN_RESOLUTION = (224, 224)\n",
        "MAX_IMAGES_PER_CLASS = {\"person\": 600, \"pet\": 600}\n",
        "\n",
        "# Map labels to unified format\n",
        "PERSON_LABELS = {\"person\", \"man\", \"woman\", \"boy\", \"girl\"}\n",
        "PET_LABELS = {\"dog\", \"cat\", \"horse\"}"
      ],
      "metadata": {
        "id": "rMzCYR64c_pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== CREATE FOLDERS ==========\n",
        "for split in [\"train\", \"validation\"]:\n",
        "    for cls in DESIRED_LABELS:\n",
        "        os.makedirs(os.path.join(SAVE_DIR, split, cls), exist_ok=True)"
      ],
      "metadata": {
        "id": "kUlObgUKZ-zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== HELPER FUNCTIONS ==========\n",
        "\n",
        "def extract_relevant_labels(result):\n",
        "    final_labels = []\n",
        "    final_bboxes = []\n",
        "    for label, bbox in zip(result[\"<OD>\"][\"labels\"], result[\"<OD>\"][\"bboxes\"]):\n",
        "        label = label.lower()\n",
        "        if label in PERSON_LABELS:\n",
        "            final_labels.append(\"person\")\n",
        "            final_bboxes.append(bbox)\n",
        "        elif label in PET_LABELS:\n",
        "            final_labels.append(\"pet\")\n",
        "            final_bboxes.append(bbox)\n",
        "    return final_labels, final_bboxes\n",
        "\n",
        "def save_image_with_annotation(image, image_path, labels, bboxes, split, used_images, counter_by_class):\n",
        "    cls = labels[0]\n",
        "    if image_path in used_images or counter_by_class[cls] >= MAX_IMAGES_PER_CLASS[cls]:\n",
        "        return False\n",
        "\n",
        "    filename = os.path.basename(image_path)\n",
        "    new_path = os.path.join(SAVE_DIR, split, cls, filename)\n",
        "    image.save(new_path)\n",
        "\n",
        "    # save annotations\n",
        "    annotation = {\"labels\": labels, \"bboxes\": bboxes}\n",
        "    with open(new_path.replace(\".jpg\", \".json\"), \"w\") as f:\n",
        "        json.dump(annotation, f)\n",
        "\n",
        "    used_images.add(image_path)\n",
        "    counter_by_class[cls] += 1\n",
        "    return True"
      ],
      "metadata": {
        "id": "8k46YinldEHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== MAIN LOOP ==========\n",
        "\n",
        "random.shuffle(list_of_images_paths)\n",
        "used_images = set()\n",
        "counter_by_class = {\"person\": 0, \"pet\": 0}\n",
        "\n",
        "for i in tqdm(range(0, len(list_of_images_paths), BATCH_SIZE), desc=\"Processing Batches\"):\n",
        "    batch_paths = list_of_images_paths[i:i + BATCH_SIZE]\n",
        "\n",
        "    for img_path in batch_paths:\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            if img.size[0] < MIN_RESOLUTION[0] or img.size[1] < MIN_RESOLUTION[1]:\n",
        "                continue\n",
        "\n",
        "            result = run_example(img)\n",
        "            labels, bboxes = extract_relevant_labels(result)\n",
        "\n",
        "            if not labels:\n",
        "                continue\n",
        "\n",
        "            primary_class = labels[0]\n",
        "            split = \"validation\" if counter_by_class[primary_class] < 100 else \"train\"\n",
        "\n",
        "            saved = save_image_with_annotation(img, img_path, labels, bboxes, split, used_images, counter_by_class)\n",
        "            if not saved:\n",
        "                continue\n",
        "\n",
        "            if all(counter_by_class[cls] >= MAX_IMAGES_PER_CLASS[cls] for cls in DESIRED_LABELS):\n",
        "                print(\"Dataset creation complete.\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {img_path} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "    if all(counter_by_class[cls] >= MAX_IMAGES_PER_CLASS[cls] for cls in DESIRED_LABELS):\n",
        "        break\n",
        "\n",
        "print(\"DONE. Final image counts:\", counter_by_class)"
      ],
      "metadata": {
        "id": "6M7RJivPZ-u3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}